# -*- coding: utf-8 -*-
"""Arpit_sarcastic_assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qrhZPi6vlYQtX5PKJwHHUoUveWJ17qk6
"""

#importing basic libraries
import pandas as pd
import numpy as np

#importing dataset from a drive
df = pd.read_json("/content/drive/My Drive/Sarcasm_Headlines_Dataset.json",lines= True)

#taking a glance of our dataset
df.head()

"""PreProcessing of a text"""

import seaborn as sns
import matplotlib.pyplot as plt
sns.countplot(df.is_sarcastic)
plt.xlabel('Label')
plt.title('Sarcasm vs Non-sarcasm')

#first thing first
#lets's check for null value
df.isnull().any(axis=0)

"""Our data has no null value so we can proceed to the next step, i.e

 remove special symobols from our headline feature
"""

import re #regex library to eliminate extra symbols

#replacing our headline column with our new column with removed symbols
df['headline'] = df['headline'].apply(lambda s : re.sub('[^a-zA-Z]', ' ', s))
# Change any white space to one space 
df['headline'] = df['headline'].apply(lambda s : re.sub('\s+',' ',s)) 
df['headline'] = df['headline'].apply(lambda x: x.lower())

# getting features and labels
features = df['headline']
labels = df['is_sarcastic']
#ignored artile_link feature as it is not contributing in classification

from nltk.stem.porter import PorterStemmer
#stemming removes suffix and get the root form of thw word

# Stemming our data by using lambda command
st = PorterStemmer()
features = features.apply(lambda x: x.split())
features = features.apply(lambda x : ' '.join([st.stem(word) for word in x]))

'''import nltk
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer   
lt = WordNetLemmatizer() 
features = features.apply(lambda x: x.split())
features = features.apply(lambda x : ' '.join([lt.lemmatize(word) for word in x]))
'''

# vectorizing the data with maximum of 5000 features
from sklearn.feature_extraction.text import TfidfVectorizer
tfid = TfidfVectorizer(max_features = 5000)
features = list(features)
features = tfid.fit_transform(features).toarray()

# getting training and testing data
from sklearn.model_selection import train_test_split
features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size = .05, random_state = 0)

# model 1:-
from sklearn.svm import LinearSVC
# Using linear support vector classifier
lsvc = LinearSVC()
# training the model
lsvc.fit(features_train, labels_train)
# getting the score of train and test data
print("Training Accuracy: ",lsvc.score(features_train, labels_train))
print("Testing Accuracy: ",lsvc.score(features_test, labels_test))

"""#let's try another machine learning model"""

#Logistic regression
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(features_train, labels_train)
print("Training accuracy",lr.score(features_train, labels_train))
print("Testing Accuracy",lr.score(features_test, labels_test))

# As we can see that Linear SVM performs slightly better than logistic regression

# We can use RNN with LSTM to get better accuracy while working with text classification.

"""## Thank You
Arpit Vijayvargia
"""

